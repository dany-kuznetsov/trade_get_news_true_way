{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_datareader import data\n",
    "from pandas_datareader._utils import RemoteDataError\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import csv\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import os.path\n",
    "from os import path\n",
    "import time\n",
    "\n",
    "# import sklearn\n",
    "# import scipy\n",
    "\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# import lightgbm as lightgbm\n",
    "# from xgboost import XGBClassifier\n",
    "# import keras\n",
    "\n",
    "# Get Started\n",
    "# https://github.com/Refinitiv-API-Samples/Article.EikonDataAPI.DotNet.Library\n",
    "# API limits\n",
    "# https://developers.refinitiv.com/en/api-catalog/eikon/eikon-data-api/documentation?content=49692&type=documentation_item\n",
    "import eikon as ek \n",
    "\n",
    "from pylab import rcParams\n",
    "plt.rcParams['figure.figsize'] = 16, 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настройки для графиков, чтобы увеличить шрифты и размеры.\n",
    "fontsize_reg = 12\n",
    "fontsize_title = 16\n",
    "from pylab import rcParams\n",
    "rcParams['figure.facecolor'] = '1'\n",
    "rcParams['figure.figsize'] = [8.0, 3.5]\n",
    "rcParams['figure.dpi'] = 80\n",
    "rcParams['savefig.dpi'] = 600\n",
    "\n",
    "rcParams['font.size'] = 12\n",
    "rcParams['legend.fontsize'] = 'large'\n",
    "rcParams['figure.titlesize'] = 'large'\n",
    "\n",
    "plt.rcParams['axes.axisbelow'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Зададим path для папки проекта и для папок файлов\n",
    "path_project_folder = 'C:/DAN/t_systems/trade_project/'\n",
    "folder_name_headlines = 'backup_headlines/'\n",
    "folder_name_logger = 'logger/'\n",
    "folder_name_for_labelling = 'for_labelling/'\n",
    "folder_name_with_labelled_data_iter1 = 'labeled_news_iter1/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_of_latest_file_in_folder(folder_name):\n",
    "    \"\"\"\n",
    "    Get all file names in selected direction.\n",
    "    Return the latest file in that directory.\n",
    "    \"\"\"\n",
    "    files_creation_dates = pd.DataFrame(data = [], columns=[\n",
    "        \"folder_name\", \"file_name\", \"num_time\", \"str_time\"\n",
    "    ])\n",
    "\n",
    "    files_creation_dates['file_name'] = os.listdir(folder_name)\n",
    "    files_creation_dates['folder_name'] = folder_name\n",
    "    files_creation_dates['file_folder_and_name'] = files_creation_dates['folder_name'] + files_creation_dates['file_name']\n",
    "    files_creation_dates\n",
    "    for i in range(0, len(files_creation_dates)):\n",
    "        file_folder_and_name = files_creation_dates['file_folder_and_name'][i]\n",
    "        files_creation_dates['num_time'][i] = os.path.getctime(file_folder_and_name)\n",
    "        files_creation_dates['str_time'][i] = time.ctime(os.path.getctime(file_folder_and_name))\n",
    "\n",
    "    latest_file_dir = files_creation_dates.sort_values(by=['num_time'], ascending=False)['file_folder_and_name'].iloc[0]\n",
    "    \n",
    "    return latest_file_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.data.path.append('/home/ec2-user/SageMaker/nltk_data')\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.stem.porter import PorterStemmer  # simple one\n",
    "from nltk.stem.snowball import SnowballStemmer  # Porter 2\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import s3fs\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss, BCELoss\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score\n",
    "from transformers import *\n",
    "from tqdm import tqdm, trange\n",
    "from ast import literal_eval\n",
    "from sklearn import metrics\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM, BertPreTrainedModel\n",
    "import boto3\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.data.path.append('/home/ec2-user/SageMaker/nltk_data')\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "# from nltk.stem.porter import PorterStemmer  # simple one\n",
    "from nltk.stem.snowball import SnowballStemmer  # Porter 2\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import ClassifierChain\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import torch\n",
    "from torch.nn import BCEWithLogitsLoss, BCELoss\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score\n",
    "from transformers import *\n",
    "from tqdm import tqdm, trange\n",
    "from ast import literal_eval\n",
    "from sklearn import metrics\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM, BertPreTrainedModel\n",
    "import boto3\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f198168f1bb4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36mcurrent_device\u001b[1;34m()\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcurrent_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;34mr\"\"\"Returns the index of a currently selected device.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 366\u001b[1;33m     \u001b[0m_lazy_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    367\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_getDevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_cuda_getDeviceCount'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_cudart\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m             raise AssertionError(\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f701d1b575e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtext_col\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe, text_col, target_col, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe[text_col]\n",
    "        self.targets = dataframe[target_col]\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
    "        }\n",
    "\t\t\n",
    "\t\t\n",
    "class BERTMultilabel(torch.nn.Module):\n",
    "    def __init__(self, num_labels, pretrained_path):\n",
    "        super(BERTMultilabel, self).__init__()\n",
    "        self.l1 = BertModel.from_pretrained(pretrained_path)\n",
    "        self.l2 = torch.nn.Dropout(0.2)\n",
    "        self.l3 = torch.nn.Linear(768, num_labels)\n",
    "        print('self.l1:', self.l1) #\n",
    "        print('self.l2:', self.l2) # \n",
    "        print('self.l3:', self.l3) # \n",
    "    \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        global bug_element# \n",
    "        _, output_1 = self.l1(ids, attention_mask = mask, token_type_ids = token_type_ids)\n",
    "        bug_element = output_1\n",
    "        print('output_1:', output_1) #\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
    "\t\n",
    "\t\n",
    "def train(model, optimizer, train_loader, device, epoch, verbose=1):\n",
    "    model.train()\n",
    "    for _, data in enumerate(train_loader, 0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        if (_%10==0 and verbose == 1) or verbose > 1:                    \n",
    "            fin_targets = targets.cpu().detach().numpy().tolist()\n",
    "            fin_outputs = torch.sigmoid(outputs).cpu().detach().numpy().tolist()\n",
    "            fin_outputs = np.array(fin_outputs) >= 0.5\n",
    "            accuracy = metrics.accuracy_score(fin_targets, fin_outputs)\n",
    "            \n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}, Acc: {accuracy}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\t\t\n",
    "\t\t\n",
    "def validation(model, loader, device, epoch):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    return fin_outputs, fin_targets\n",
    "\t\n",
    "\t\n",
    "\t\n",
    "def run_experiment(train_df, val_df, text_col, verbose=1):\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "    val_df = val_df.reset_index(drop=True)\n",
    "    print(train_df.shape, val_df.shape)\n",
    "    \n",
    "    tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL)\n",
    "    \n",
    "    training_set = CustomDataset(train_df, text_col, 'list_label', tokenizer, MAX_LEN)\n",
    "    validation_set = CustomDataset(val_df, text_col, 'list_label', tokenizer, MAX_LEN)\n",
    "\n",
    "\n",
    "    train_iterations = len(training_set)/BATCH_SIZE\n",
    "    val_iterations = len(validation_set)/BATCH_SIZE\n",
    "    \n",
    "    train_params = {'batch_size': BATCH_SIZE,\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "    val_params = {'batch_size': BATCH_SIZE,\n",
    "                    'shuffle': False,\n",
    "                    'num_workers': 0\n",
    "                    }\n",
    "\n",
    "    train_loader = DataLoader(training_set, **train_params)\n",
    "    val_loader = DataLoader(validation_set, **val_params)\n",
    "    \n",
    "    model = BERTMultilabel(NUM_LABELS, PRETRAINED_PATH)\n",
    "    model.to(device)\n",
    "    \n",
    "    print('Modelling')\n",
    "    \n",
    "    optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        train(model, optimizer, train_loader, device, epoch, verbose)\n",
    "    \n",
    "    epoch = range(EPOCHS)[-1]\n",
    "\n",
    "    outputs, targets = validation(model, val_loader, device, epoch)\n",
    "    outputs = np.array(outputs) >= 0.5\n",
    "\n",
    "    accuracy = metrics.accuracy_score(targets, outputs)\n",
    "    f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n",
    "    f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n",
    "\n",
    "    print(f'Epoch: {epoch}, Acc: {accuracy}')\n",
    "    #     print(f\"Accuracy Score = {accuracy}\")\n",
    "    #     print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
    "    #     print(f\"F1 Score (Macro) = {f1_score_macro}\")\n",
    "    print(metrics.classification_report(targets, outputs))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Прочитаем файлы с разметкой и соединим их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Из всех колонок в файлах разметки выберем только колонки разметки\n",
    "cols_of_labelling = [\n",
    "#     'Unnamed: 0',\n",
    "#     'storyId',\n",
    "#     'versionCreated',\n",
    "#     'sourceCode',\n",
    "#     'ric',\n",
    "#     'story_lang',\n",
    "#     'Analyst_Rating',\n",
    "#     'Asset_Sale',\n",
    "#     'Collaboration',\n",
    "#     'Contract_-_Agreement_-_Deal',\n",
    "#     'Credit_Debt_Rating',\n",
    "#     'Dividend',\n",
    "#     'Eco_Issues',\n",
    "#     'Executive_Change',\n",
    "#     'Financial_Results',\n",
    "#     'Investigation',\n",
    "#     'Investment',\n",
    "#     'Lawsuit',\n",
    "#     'Merger_and_Acquisition',\n",
    "#     'Price_Target',\n",
    "#     'Product_Update',\n",
    "#     'Security_Protection',\n",
    "#     'Settlement',\n",
    "#     'Stock_Buyback',\n",
    "#     'Workforce_Change',\n",
    "#     'number_of_types_detected',\n",
    "#     'headline',\n",
    "#     'story_wo_html',\n",
    "    'my_label_Analyst_Rating',\n",
    "#    'my_label_Asset_Sale',\n",
    "    'my_label_Collaboration',\n",
    "    'my_label_Contract_-_Agreement_-_Deal',\n",
    "    'my_label_Credit_Debt_Rating',\n",
    "    'my_label_Dividend',\n",
    "    'my_label_Eco_Issues',\n",
    "    'my_label_Executive_Change',\n",
    "    'my_label_Financial_Results',\n",
    "    'my_label_Investigation',\n",
    "    'my_label_Investment',\n",
    "    'my_label_Lawsuit',\n",
    "#    'my_label_Merger_and_Acquisition',\n",
    "    'my_label_Price_Target',\n",
    "    'my_label_Product_Update',\n",
    "    'my_label_Security_Protection',\n",
    "    'my_label_Settlement',\n",
    "    'my_label_Stock_Buyback',\n",
    "    'my_label_Workforce_Change',\n",
    "#     'my_label_digest'\n",
    "]\n",
    "\n",
    "len(cols_of_labelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_name_in_folder(folder_name):\n",
    "    files_creation_dates = pd.DataFrame(data = [], columns=[\n",
    "        \"folder_name\", \"file_name\", \"num_time\", \"str_time\"\n",
    "    ])\n",
    "\n",
    "    files_creation_dates['file_name'] = os.listdir(folder_name)\n",
    "    files_creation_dates['folder_name'] = folder_name\n",
    "    files_creation_dates['file_folder_and_name'] = files_creation_dates['folder_name'] + files_creation_dates['file_name']\n",
    "    files_creation_dates\n",
    "    for i in range(0, len(files_creation_dates)):\n",
    "        file_folder_and_name = files_creation_dates['file_folder_and_name'][i]\n",
    "        files_creation_dates['num_time'][i] = os.path.getctime(file_folder_and_name)\n",
    "        files_creation_dates['str_time'][i] = time.ctime(os.path.getctime(file_folder_and_name))\n",
    "        \n",
    "    return list(files_creation_dates['file_folder_and_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создадим лист только с названиями экселек с разметкой, будем его итерировать\n",
    "import re\n",
    "\n",
    "regex_for_filter = re.compile(r'/for_labelling(.*).xlsx')\n",
    "\n",
    "files_list_labelling = list(filter(\n",
    "    regex_for_filter.search, \n",
    "    get_files_name_in_folder(\n",
    "        folder_name=path_project_folder+folder_name_with_labelled_data_iter1)\n",
    "))\n",
    "\n",
    "len(files_list_labelling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 C:/DAN/t_systems/trade_project/labeled_news_iter1/for_labelling2_Merger_and_Acquisition.xlsx\n",
      "100 101 C:/DAN/t_systems/trade_project/labeled_news_iter1/for_labelling_Analyst_Rating.xlsx\n",
      "0 101 C:/DAN/t_systems/trade_project/labeled_news_iter1/for_labelling_Asset_Sale.xlsx\n",
      "43 144 C:/DAN/t_systems/trade_project/labeled_news_iter1/for_labelling_Collaboration.xlsx\n",
      "51 195 C:/DAN/t_systems/trade_project/labeled_news_iter1/for_labelling_Contract_-_Agreement_-_Deal.xlsx\n",
      "22 217 C:/DAN/t_systems/trade_project/labeled_news_iter1/for_labelling_Credit_Debt_Rating.xlsx\n",
      "64 281 C:/DAN/t_systems/trade_project/labeled_news_iter1/for_labelling_Dividend.xlsx\n",
      "44 325 C:/DAN/t_systems/trade_project/labeled_news_iter1/for_labelling_Eco_Issues.xlsx\n",
      "37 362 C:/DAN/t_systems/trade_project/labeled_news_iter1/for_labelling_Executive_Change.xlsx\n",
      "80 442 C:/DAN/t_systems/trade_project/labeled_news_iter1/for_labelling_Financial_Results.xlsx\n",
      "64 506 C:/DAN/t_systems/trade_project/labeled_news_iter1/for_labelling_Investigation.xlsx\n",
      "28 534 C:/DAN/t_systems/trade_project/labeled_news_iter1/for_labelling_Investment.xlsx\n",
      "42 576 C:/DAN/t_systems/trade_project/labeled_news_iter1/for_labelling_Lawsuit.xlsx\n",
      "3 579 C:/DAN/t_systems/trade_project/labeled_news_iter1/for_labelling_Merger_and_Acquisition.xlsx\n",
      "52 631 C:/DAN/t_systems/trade_project/labeled_news_iter1/for_labelling_Price_Target.xlsx\n",
      "35 666 C:/DAN/t_systems/trade_project/labeled_news_iter1/for_labelling_Product_Update.xlsx\n",
      "35 701 C:/DAN/t_systems/trade_project/labeled_news_iter1/for_labelling_Security_Protection.xlsx\n",
      "33 734 C:/DAN/t_systems/trade_project/labeled_news_iter1/for_labelling_Settlement.xlsx\n",
      "47 781 C:/DAN/t_systems/trade_project/labeled_news_iter1/for_labelling_Stock_Buyback.xlsx\n",
      "70 851 C:/DAN/t_systems/trade_project/labeled_news_iter1/for_labelling_Workforce_Change.xlsx\n"
     ]
    }
   ],
   "source": [
    "labelling_all_df = pd.DataFrame(data = [], columns=[])\n",
    "\n",
    "for file_name_now in files_list_labelling:\n",
    "    # Ранее мы уже создали список файлов с полным путём, проитерируем его\n",
    "    labelled_slice_now = pd.read_excel(file_name_now)\n",
    "\n",
    "    # Выберем только столбцы с разметкой и ID новости\n",
    "    try:\n",
    "        # Постараем подгрузить и столбец 'my_label_digest'\n",
    "        # Он означает, что в тексте собраны сразу несколько разнообразных новостей\n",
    "        labelled_slice_now = labelled_slice_now[\n",
    "            ['storyId'] + cols_of_labelling + ['my_label_digest']\n",
    "        ]\n",
    "    except Exception as e:\n",
    "        # Если не получилось, попробуем без этого столбца\n",
    "        # print(e, end=' ')\n",
    "        labelled_slice_now = labelled_slice_now[\n",
    "            ['storyId'] + cols_of_labelling\n",
    "        ]        \n",
    "\n",
    "    # Заменим пропуски на нули. Ноль - новость не про данный тип события\n",
    "    labelled_slice_now = labelled_slice_now\n",
    "    labelled_slice_now['labels_sum'] = labelled_slice_now.sum(axis=1)\n",
    "    labelled_slice_now = labelled_slice_now[labelled_slice_now['labels_sum'] > 0].reset_index(drop=True)\n",
    "\n",
    "    print(labelled_slice_now.shape[0], end=' ')\n",
    "    \n",
    "    labelling_all_df = labelling_all_df.append(labelled_slice_now, ignore_index = True)\n",
    "    \n",
    "    print(labelling_all_df.shape[0], end=' ')\n",
    "    \n",
    "    print(file_name_now)\n",
    "\n",
    "labelling_all_df = labelling_all_df.replace(np.nan, 0)\n",
    "labelling_all_df = labelling_all_df.drop(columns=['labels_sum'])\n",
    "\n",
    "# Заменим все двойки и едницы на единицы, оставив лишь два возможные метки\n",
    "labelling_all_df = labelling_all_df.replace(2, 1)\n",
    "labelling_all_df = labelling_all_df.replace(1, 1)\n",
    "labelling_all_df = labelling_all_df.replace(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "918.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\n",
    "    labelling_all_df[cols_of_labelling + ['my_label_digest']].sum().sum()\n",
    ")\n",
    "(labelling_all_df[cols_of_labelling + ['my_label_digest']].sum()).plot(kind='barh', width=0.8, figsize=(8,6))\n",
    "plt.title('Number of labelled news per class (event-type)')\n",
    "plt.grid(True, axis='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>storyId</th>\n",
       "      <th>my_label_Analyst_Rating</th>\n",
       "      <th>my_label_Collaboration</th>\n",
       "      <th>my_label_Contract_-_Agreement_-_Deal</th>\n",
       "      <th>my_label_Credit_Debt_Rating</th>\n",
       "      <th>my_label_Dividend</th>\n",
       "      <th>my_label_Eco_Issues</th>\n",
       "      <th>my_label_Executive_Change</th>\n",
       "      <th>my_label_Financial_Results</th>\n",
       "      <th>my_label_Investigation</th>\n",
       "      <th>my_label_Investment</th>\n",
       "      <th>my_label_Lawsuit</th>\n",
       "      <th>my_label_Price_Target</th>\n",
       "      <th>my_label_Product_Update</th>\n",
       "      <th>my_label_Security_Protection</th>\n",
       "      <th>my_label_Settlement</th>\n",
       "      <th>my_label_Stock_Buyback</th>\n",
       "      <th>my_label_Workforce_Change</th>\n",
       "      <th>my_label_digest</th>\n",
       "      <th>list_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>urn:newsml:reuters.com:20191209:nNRAaepw3s:1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          storyId  my_label_Analyst_Rating  \\\n",
       "850  urn:newsml:reuters.com:20191209:nNRAaepw3s:1                      0.0   \n",
       "\n",
       "     my_label_Collaboration  my_label_Contract_-_Agreement_-_Deal  \\\n",
       "850                     0.0                                   0.0   \n",
       "\n",
       "     my_label_Credit_Debt_Rating  my_label_Dividend  my_label_Eco_Issues  \\\n",
       "850                          0.0                0.0                  0.0   \n",
       "\n",
       "     my_label_Executive_Change  my_label_Financial_Results  \\\n",
       "850                        0.0                         0.0   \n",
       "\n",
       "     my_label_Investigation  my_label_Investment  my_label_Lawsuit  \\\n",
       "850                     0.0                  0.0               0.0   \n",
       "\n",
       "     my_label_Price_Target  my_label_Product_Update  \\\n",
       "850                    0.0                      0.0   \n",
       "\n",
       "     my_label_Security_Protection  my_label_Settlement  \\\n",
       "850                           0.0                  0.0   \n",
       "\n",
       "     my_label_Stock_Buyback  my_label_Workforce_Change  my_label_digest  \\\n",
       "850                     0.0                        1.0              0.0   \n",
       "\n",
       "                                            list_label  \n",
       "850  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelling_all_df['list_label'] = labelling_all_df[cols_of_labelling + ['my_label_digest']].values.tolist()\n",
    "labelling_all_df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "storyId                                 urn:newsml:reuters.com:20201022:nNDL3RTZ1L:1ur...\n",
       "my_label_Analyst_Rating                                                               107\n",
       "my_label_Collaboration                                                                 57\n",
       "my_label_Contract_-_Agreement_-_Deal                                                   35\n",
       "my_label_Credit_Debt_Rating                                                            22\n",
       "my_label_Dividend                                                                      49\n",
       "my_label_Eco_Issues                                                                    51\n",
       "my_label_Executive_Change                                                              46\n",
       "my_label_Financial_Results                                                             86\n",
       "my_label_Investigation                                                                 70\n",
       "my_label_Investment                                                                    19\n",
       "my_label_Lawsuit                                                                       55\n",
       "my_label_Price_Target                                                                  31\n",
       "my_label_Product_Update                                                                67\n",
       "my_label_Security_Protection                                                           17\n",
       "my_label_Settlement                                                                    33\n",
       "my_label_Stock_Buyback                                                                 46\n",
       "my_label_Workforce_Change                                                              71\n",
       "my_label_digest                                                                        56\n",
       "list_label                              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...\n",
       "dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelling_all_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = path_project_folder + 'labelling_all_df' + '.csv'\n",
    "labelling_all_df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['storyId', 'my_label_Analyst_Rating', 'my_label_Asset_Sale',\n",
       "       'my_label_Collaboration', 'my_label_Contract_-_Agreement_-_Deal',\n",
       "       'my_label_Credit_Debt_Rating', 'my_label_Dividend',\n",
       "       'my_label_Eco_Issues', 'my_label_Executive_Change',\n",
       "       'my_label_Financial_Results', 'my_label_Investigation',\n",
       "       'my_label_Investment', 'my_label_Lawsuit',\n",
       "       'my_label_Merger_and_Acquisition', 'my_label_Price_Target',\n",
       "       'my_label_Product_Update', 'my_label_Security_Protection',\n",
       "       'my_label_Settlement', 'my_label_Stock_Buyback',\n",
       "       'my_label_Workforce_Change', 'my_label_digest', 'list_label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelling_all_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разделим на train, validation и test выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(257644, 109)\n"
     ]
    }
   ],
   "source": [
    "# Може подгрузить csv, если хотим откатиться и не пересчитывать\n",
    "key_words_detection_df = pd.read_csv(path_project_folder + \"key_words_detection_df.csv\")\n",
    "print(key_words_detection_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Добавим тексты к нашему датасету с разметкой\n",
    "lower_labelled_texts_list_types = pd.merge(\n",
    "    key_words_detection_df, \n",
    "    labelling_all_df[['storyId', 'list_label']], \n",
    "    how=\"right\", \n",
    "    on='storyId'\n",
    ").drop_duplicates(\n",
    "    subset=['storyId'], \n",
    "    keep='first'\n",
    ").reset_index(drop=True) # [['story_wo_html', 'list_label']]\n",
    "\n",
    "lower_labelled_texts_list_types['story_wo_html_lower'] = lower_labelled_texts_list_types['story_wo_html'].str.lower()\n",
    "lower_labelled_texts_list_types = lower_labelled_texts_list_types.drop(columns=['story_wo_html'])\n",
    "\n",
    "lower_labelled_texts_list_types = lower_labelled_texts_list_types[['story_wo_html_lower', 'list_label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_labelled_texts_one_type = pd.merge(\n",
    "    key_words_detection_df, \n",
    "    labelling_all_df[['storyId', 'my_label_Executive_Change']], \n",
    "    how=\"right\", \n",
    "    on='storyId'\n",
    ").drop_duplicates(\n",
    "    subset=['storyId'], \n",
    "    keep='first'\n",
    ").reset_index(drop=True)[['story_wo_html', 'my_label_Executive_Change']]\n",
    "\n",
    "lower_labelled_texts_one_type['story_wo_html_lower'] = lower_labelled_texts_one_type['story_wo_html'].str.lower()\n",
    "lower_labelled_texts_one_type = lower_labelled_texts_one_type.drop(columns=['story_wo_html'])\n",
    "\n",
    "lower_labelled_texts_one_type = lower_labelled_texts_one_type[['story_wo_html_lower', 'my_label_Executive_Change']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 18\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    'Number of labels:',\n",
    "    len(lower_labelled_texts_list_types['list_label'][1])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[97.0,\n",
       " 52.0,\n",
       " 29.0,\n",
       " 20.0,\n",
       " 46.0,\n",
       " 47.0,\n",
       " 43.0,\n",
       " 82.0,\n",
       " 62.0,\n",
       " 18.0,\n",
       " 46.0,\n",
       " 26.0,\n",
       " 66.0,\n",
       " 17.0,\n",
       " 28.0,\n",
       " 42.0,\n",
       " 68.0,\n",
       " 22.0]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = lower_labelled_texts_list_types['list_label'].apply(pd.Series)\n",
    "\n",
    "df['story_wo_html_lower'] = lower_labelled_texts_list_types['story_wo_html_lower']\n",
    "list(df.sum()[:len(cols_of_labelling) + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_labelled_texts_list_types.to_csv(path_project_folder + 'lower_labelled_texts_list_types.csv')\n",
    "lower_labelled_texts_one_type.to_csv(path_project_folder + 'lower_labelled_texts_one_type.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Попытка сделать по коду Влада"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(532, 2) (229, 2)\n"
     ]
    }
   ],
   "source": [
    "random=69420\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    lower_labelled_texts_list_types, \n",
    "    test_size=0.30, \n",
    "    random_state=random)\n",
    "print(train_df.shape, val_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELS = 20\n",
    "PRETRAINED_PATH = 'bert-base-multilingual-uncased' # C:/DAN/t_systems/trade_project/BERT_pretrained/multilingual_L-12_H-768_A-12/\n",
    "PRE_TRAINED_MODEL = 'bert-base-multilingual-uncased' \n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 0.1\n",
    "EPOCHS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "\n",
    "model = BertForMaskedLM.from_pretrained(\"bert-base-multilingual-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(532, 2) (229, 2)\n",
      "self.l1: BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (word_embeddings): Embedding(105879, 768, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 768)\n",
      "    (token_type_embeddings): Embedding(2, 768)\n",
      "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder): BertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (1): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (2): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (3): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (4): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (5): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (6): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (7): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (8): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (9): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (10): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (11): BertLayer(\n",
      "        (attention): BertAttention(\n",
      "          (self): BertSelfAttention(\n",
      "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): BertSelfOutput(\n",
      "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (intermediate): BertIntermediate(\n",
      "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        )\n",
      "        (output): BertOutput(\n",
      "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      ")\n",
      "self.l2: Dropout(p=0.2, inplace=False)\n",
      "self.l3: Linear(in_features=768, out_features=20, bias=True)\n",
      "Modelling\n",
      "output_1: pooler_output\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "dropout(): argument 'input' (position 1) must be Tensor, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-c3d85e77b0c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mval_df\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtext_col\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'story_wo_html_lower'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m )\n",
      "\u001b[1;32m<ipython-input-65-f701d1b575e1>\u001b[0m in \u001b[0;36mrun_experiment\u001b[1;34m(train_df, val_df, text_col, verbose)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[0mepoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-65-f701d1b575e1>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, optimizer, train_loader, device, epoch, verbose)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'targets'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-65-f701d1b575e1>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, ids, mask, token_type_ids)\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mbug_element\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'output_1:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0moutput_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    726\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 727\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\torch\\nn\\modules\\dropout.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m    981\u001b[0m     return (_VF.dropout_(input, p, training)\n\u001b[0;32m    982\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 983\u001b[1;33m             else _VF.dropout(input, p, training))\n\u001b[0m\u001b[0;32m    984\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: dropout(): argument 'input' (position 1) must be Tensor, not str"
     ]
    }
   ],
   "source": [
    "model = run_experiment(\n",
    "    train_df, \n",
    "    val_df, \n",
    "    text_col = 'story_wo_html_lower',\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TypeError: dropout(): argument 'input' (position 1) must be Tensor, not str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pooler_output'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bug_element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Неудачная попытка сделать по чужому коду"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformers as ppb # pytorch transformers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5690d1076d2a4e198fac5d3952d8637c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d777bae501964ea9a8b6536e4d34d34d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/442 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db994aa3f2cc413c94840580a4af288a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = df['story_wo_html_lower'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True, truncation=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'padded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-a908d2e17274>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minput_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mlast_hidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'padded' is not defined"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor(np.array(padded))\n",
    "\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Попытка 1 после Москвы построить нейронку"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/huggingface/multi-label-text-classification-using-bert-the-mighty-transformer-69714fa3fb3d\n",
    "\n",
    "The model is also pre-trained on two unsupervised tasks, masked language modeling and next sentence prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert input leght limitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will convert the InputExample to the feature that is understood by BERT. The feature will be represented by class InputFeatures.\n",
    "- input_ids: list of numerical ids for the tokenised text\n",
    "- input_mask: will be set to 1 for real tokens and 0 for the padding tokens\n",
    "- segment_ids: for our case, this will be set to the list of ones\n",
    "- label_ids: one-hot encoded labels for the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PreTrainedBertModel - не знает, что это за сущность"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, labels=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            labels: (Optional) [string]. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.labels = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will adapt BertForSequenceClassification class to cater for multi-label classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nAutoModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-63ede9b837a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bert-base-uncased\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bert-base-uncased\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Hello world!\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"pt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\utils\\dummy_pt_objects.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m         \u001b[0mrequires_pytorch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\file_utils.py\u001b[0m in \u001b[0;36mrequires_pytorch\u001b[1;34m(obj)\u001b[0m\n\u001b[0;32m    459\u001b[0m     \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"__name__\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 461\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPYTORCH_IMPORT_ERROR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: \nAutoModel requires the PyTorch library but it was not found in your environment. Checkout the instructions on the\ninstallation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForMultiLabelSequenceClassification(BertModel): # PreTrainedBertModel\n",
    "    \"\"\"BERT model for classification.\n",
    "    This module is composed of the BERT model with a linear layer on top of\n",
    "    the pooled output.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, num_labels=2):\n",
    "        super(BertForMultiLabelSequenceClassification, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, num_labels)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))\n",
    "            return loss\n",
    "        else:\n",
    "            return logits\n",
    "        \n",
    "    def freeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 21 # ДОБАВИЛ Я\n",
    "\n",
    "all_labels = [ # ДОБАВИЛ Я\n",
    "    'my_label_Analyst_Rating', 'my_label_Asset_Sale',\n",
    "    'my_label_Collaboration', 'my_label_Contract_-_Agreement_-_Deal',\n",
    "    'my_label_Credit_Debt_Rating', 'my_label_Dividend',\n",
    "    'my_label_Eco_Issues', 'my_label_Executive_Change',\n",
    "    'my_label_Financial_Results', 'my_label_Investigation',\n",
    "    'my_label_Investment', 'my_label_Lawsuit',\n",
    "    'my_label_Merger_and_Acquisition', 'my_label_Price_Target',\n",
    "    'my_label_Product_Update', 'my_label_Security_Protection',\n",
    "    'my_label_Settlement', 'my_label_Stock_Buyback',\n",
    "    'my_label_Workforce_Change', 'my_label_digest'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-b4282454924c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mfpr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_logits\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mroc_auc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mauc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "from torch import Tensor\n",
    "\n",
    "def accuracy_thresh(y_pred:Tensor, y_true:Tensor, thresh:float=0.5, sigmoid:bool=True):\n",
    "    \"Compute accuracy when `y_pred` and `y_true` are the same size.\"\n",
    "    if sigmoid: y_pred = y_pred.sigmoid()\n",
    "\n",
    "    return np.mean(((y_pred>thresh)==y_true.byte()).float().cpu().numpy(), axis=1).sum()\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(num_labels):\n",
    "    fpr[i], tpr[i], _ = roc_curve(all_labels[:, i], all_logits[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Compute micro-average ROC curve and ROC area\n",
    "fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(all_labels.ravel(), all_logits.ravel())\n",
    "roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Попытка 2 после Москвы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/notebooks?sortBy=voteCount&searchQuery=bert\n",
    "\n",
    "Попробовал код ниже - не подгружается библиотека fastai.callbacks\n",
    "\n",
    "Рекомендуют сделать среду с помощью конды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import *\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python37\\lib\\site-packages\\fastbook\\__init__.py:18: UserWarning: Missing `graphviz` - please run `conda install fastbook`\n",
      "  except ModuleNotFoundError: warn(\"Missing `graphviz` - please run `conda install fastbook`\")\n"
     ]
    }
   ],
   "source": [
    "import fastbook\n",
    "fastbook.setup_book()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.text import *\n",
    "\n",
    "# ВОТ ЭТА СТРОЧКА НЕ ПОДГРУЖАЕТСЯ\n",
    "from fastai.callbacks import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)\n",
    "    \n",
    "    def set(self, key, val):\n",
    "        self[key] = val\n",
    "        setattr(self, key, val)\n",
    "\n",
    "config = Config(\n",
    "    testing=False,\n",
    "    bert_model_name=\"bert-base-uncased\",\n",
    "    max_lr=3e-5,\n",
    "    epochs=4,\n",
    "    use_fp16=True,\n",
    "    bs=32,\n",
    "    discriminative=False,\n",
    "    max_seq_len=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "bert_tok = BertTokenizer.from_pretrained(\n",
    "    config.bert_model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _join_texts(texts:Collection[str], mark_fields:bool=False, sos_token:Optional[str]='BOS'):\n",
    "    \"\"\"Borrowed from fast.ai source\"\"\"\n",
    "    if not isinstance(texts, np.ndarray): texts = np.array(texts)\n",
    "    if is1d(texts): texts = texts[:,None]\n",
    "    df = pd.DataFrame({i:texts[:,i] for i in range(texts.shape[1])})\n",
    "    text_col = f'{FLD} {1} ' + df[0].astype(str) if mark_fields else df[0].astype(str)\n",
    "    if sos_token is not None: text_col = f\"{sos_token} \" + text_col\n",
    "    for i in range(1,len(df.columns)):\n",
    "        #text_col += (f' {FLD} {i+1} ' if mark_fields else ' ') + df[i]\n",
    "        text_col += (f' {FLD} {i+1} ' if mark_fields else ' ') + df[i].astype(str)\n",
    "    return text_col.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-822cccbb9e0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mFastAiBertTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseTokenizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[1;34m\"\"\"Wrapper around BertTokenizer to be compatible with fast.ai\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pretrained_tokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_seq_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BaseTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "class FastAiBertTokenizer(BaseTokenizer):\n",
    "    \"\"\"Wrapper around BertTokenizer to be compatible with fast.ai\"\"\"\n",
    "    def __init__(self, tokenizer: BertTokenizer, max_seq_len: int=128, **kwargs):\n",
    "        self._pretrained_tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self\n",
    "\n",
    "    def tokenizer(self, t:str) -> List[str]:\n",
    "        \"\"\"Limits the maximum sequence length\"\"\"\n",
    "        return [\"[CLS]\"] + self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len - 2] + [\"[SEP]\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Попытка 3 после Москвы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1Y4o3jh3ZH70tl6mCd76vz_IxX23biCPP#scrollTo=cfNIhN19te3N\n",
    "\n",
    "Мне кажется, не сработало на моих данных, потому что там размечают предложения и вставляют PAD SEP CLS куда надо, если бы текст был бы просто предложением"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Попытка 4 после Москвы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=wG2J_MJEjSQ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Попытка 5 после Москвы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = path_project_folder + 'toxic_comments/train.csv'\n",
    "test_path = path_project_folder + 'toxic_comments/test.csv'\n",
    "test_labels_path = path_project_folder + 'toxic_comments/test_labels.csv'\n",
    "subm_path = path_project_folder + 'toxic_comments/sample_submission.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "df_test_labels = pd.read_csv(test_labels_path)\n",
    "df_test_labels = df_test_labels.set_index('id')\n",
    "\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.keras.layers.experimental.preprocessing'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-da7cc7d19269>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mbert_model_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'bert-base-uncased'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     raise ImportError(\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[1;34m'Keras requires TensorFlow 2.2 or higher. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         'Install TensorFlow via `pip install tensorflow`')\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name, do_lower_case=True)\n",
    "MAX_LEN = 128\n",
    "\n",
    "def tokenize_sentences(sentences, tokenizer, max_seq_len = 128):\n",
    "    tokenized_sentences = []\n",
    "\n",
    "    for sentence in tqdm(sentences):\n",
    "        tokenized_sentence = tokenizer.encode(\n",
    "                            sentence,                  # Sentence to encode.\n",
    "                            add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                            max_length = max_seq_len,  # Truncate all sentences.\n",
    "                    )\n",
    "        \n",
    "        tokenized_sentences.append(tokenized_sentence)\n",
    "\n",
    "    return tokenized_sentences\n",
    "\n",
    "def create_attention_masks(tokenized_and_padded_sentences):\n",
    "    attention_masks = []\n",
    "\n",
    "    for sentence in tokenized_and_padded_sentences:\n",
    "        att_mask = [int(token_id > 0) for token_id in sentence]\n",
    "        attention_masks.append(att_mask)\n",
    "\n",
    "    return np.asarray(attention_masks)\n",
    "\n",
    "input_ids = tokenize_sentences(df_train['comment_text'], tokenizer, MAX_LEN)\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", value=0, truncating=\"post\", padding=\"post\")\n",
    "attention_masks = create_attention_masks(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labels =  df_train[label_cols].values\n",
    "\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, random_state=0, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels, random_state=0, test_size=0.1)\n",
    "\n",
    "train_size = len(train_inputs)\n",
    "validation_size = len(validation_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NR_EPOCHS = 1\n",
    "\n",
    "def create_dataset(data_tuple, epochs=1, batch_size=32, buffer_size=10000, train=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data_tuple)\n",
    "    if train:\n",
    "        dataset = dataset.shuffle(buffer_size=buffer_size)\n",
    "    dataset = dataset.repeat(epochs)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    if train:\n",
    "        dataset = dataset.prefetch(1)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "train_dataset = create_dataset((train_inputs, train_masks, train_labels), epochs=NR_EPOCHS, batch_size=BATCH_SIZE)\n",
    "validation_dataset = create_dataset((validation_inputs, validation_masks, validation_labels), epochs=NR_EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFBertModel\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "class BertClassifier(tf.keras.Model):    \n",
    "    def __init__(self, bert: TFBertModel, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.classifier = Dense(num_classes, activation='sigmoid')\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n",
    "        outputs = self.bert(input_ids,\n",
    "                               attention_mask=attention_mask,\n",
    "                               token_type_ids=token_type_ids,\n",
    "                               position_ids=position_ids,\n",
    "                               head_mask=head_mask)\n",
    "        cls_output = outputs[1]\n",
    "        cls_output = self.classifier(cls_output)\n",
    "                \n",
    "        return cls_output\n",
    "\n",
    "model = BertClassifier(TFBertModel.from_pretrained(bert_model_name), len(label_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import create_optimizer\n",
    "\n",
    "steps_per_epoch = train_size // BATCH_SIZE\n",
    "validation_steps = validation_size // BATCH_SIZE\n",
    "\n",
    "# | Loss Function\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "validation_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "\n",
    "# | Optimizer (with 1-cycle-policy)\n",
    "warmup_steps = steps_per_epoch // 3\n",
    "total_steps = steps_per_epoch * NR_EPOCHS - warmup_steps\n",
    "optimizer = create_optimizer(init_lr=2e-5, num_train_steps=total_steps, num_warmup_steps=warmup_steps)\n",
    "\n",
    "# | Metrics\n",
    "train_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(label_cols))]\n",
    "validation_auc_metrics = [tf.keras.metrics.AUC() for i in range(len(label_cols))]\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, token_ids, masks, labels):\n",
    "    labels = tf.dtypes.cast(labels, tf.float32)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(token_ids, attention_mask=masks)\n",
    "        loss = loss_object(labels, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables), 1.0)\n",
    "\n",
    "    train_loss(loss)\n",
    "\n",
    "    for i, auc in enumerate(train_auc_metrics):\n",
    "        auc.update_state(labels[:,i], predictions[:,i])\n",
    "        \n",
    "@tf.function\n",
    "def validation_step(model, token_ids, masks, labels):\n",
    "    labels = tf.dtypes.cast(labels, tf.float32)\n",
    "\n",
    "    predictions = model(token_ids, attention_mask=masks, training=False)\n",
    "    v_loss = loss_object(labels, predictions)\n",
    "\n",
    "    validation_loss(v_loss)\n",
    "    for i, auc in enumerate(validation_auc_metrics):\n",
    "        auc.update_state(labels[:,i], predictions[:,i])\n",
    "                                              \n",
    "def train(model, train_dataset, val_dataset, train_steps_per_epoch, val_steps_per_epoch, epochs):\n",
    "    for epoch in range(epochs):\n",
    "        print('=' * 50, f\"EPOCH {epoch}\", '=' * 50)\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        for i, (token_ids, masks, labels) in enumerate(tqdm(train_dataset, total=train_steps_per_epoch)):\n",
    "            train_step(model, token_ids, masks, labels)\n",
    "            if i % 1000 == 0:\n",
    "                print(f'\\nTrain Step: {i}, Loss: {train_loss.result()}')\n",
    "                for i, label_name in enumerate(label_cols):\n",
    "                    print(f\"{label_name} roc_auc {train_auc_metrics[i].result()}\")\n",
    "                    train_auc_metrics[i].reset_states()\n",
    "        \n",
    "        for i, (token_ids, masks, labels) in enumerate(tqdm(val_dataset, total=val_steps_per_epoch)):\n",
    "            validation_step(model, token_ids, masks, labels)\n",
    "\n",
    "        print(f'\\nEpoch {epoch+1}, Validation Loss: {validation_loss.result()}, Time: {time.time()-start}\\n')\n",
    "\n",
    "        for i, label_name in enumerate(label_cols):\n",
    "            print(f\"{label_name} roc_auc {validation_auc_metrics[i].result()}\")\n",
    "            validation_auc_metrics[i].reset_states()\n",
    "\n",
    "        print('\\n')\n",
    "\n",
    "        \n",
    "train(model, train_dataset, validation_dataset, train_steps_per_epoch=steps_per_epoch, val_steps_per_epoch=validation_steps, epochs=NR_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
